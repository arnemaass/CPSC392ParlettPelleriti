{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "## GENERAL INSTRUCTIONS:\n",
    "\n",
    "- CLEARLY mark where you are answering each question (all questions must be answered in Markdown cells, NOT as comments in code cells)\n",
    "- Show all code necessary for the analysis, but remove superfluous code\n",
    "- Check that your final PDF does not have code/markdown cutoff\n",
    "\n",
    "---\n",
    "\n",
    "Use the [Word Recall Data Set](https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/HW2.csv) on GitHub to build the classification models described below.\n",
    "\n",
    "This data is *synthetic data* based on data from [this](https://osf.io/5yp4r/) study (Gender and State are completely fabricated). Pretend that you're working with a group of researchers to build a model to predict what words people will remember.\n",
    "\n",
    "## Variable Descriptions:\n",
    "- `Position`: the order/position of the word in the task (the lower the number the earlier in the task)\n",
    "- `correct`: 0 if the subject did NOT recall the word, 1 if they did.\n",
    "- `WordList`: 0 if the word was in the first list, 1 if it was in the second.\n",
    "- `Trial.Number`: which trial (starting at 1 for the first trial) the word was shown in. Lower numbers mean the word was shown earlier. \n",
    "- `out_degree`: . Words of high degree centrality are associated with more words in free association norms than those of low degree centrality. Out-degree is the number of connections that originate at a vertex and point outward to other vertices. See [here](https://osf.io/5yp4r/) for more.\n",
    "- `in_degree`:  . Words of high degree centrality are associated with more words in free association norms than those of low degree centrality. In-degree is the number of connections that point inward at a vertex. See [here](https://osf.io/5yp4r/) for more.\n",
    "- `Length`: the length of the word (number of letters)\n",
    "- `Log_Freq_HAL`: the log frequency of the word in various corpera. Essentially, it measures how commonly the word is used (on a log scale).\n",
    "- `Ortho.N`: Number of Orthographic Neighbours (*\"The orthographic neighbourhood of a word is the set of words that can be formed by substituting a single letter in a target word. For example, the neighbourhood of sleet is fleet, sheet, skeet, sweet, slept, sleek, and sleep\"* ([Parker et al, 2021](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8088209/#:~:text=The%20orthographic%20neighbourhood%20of%20a,slept%2C%20sleek%2C%20and%20sleep.))).\n",
    "- `Phono.N`: Number of Phonological Neighbours (*\"Phonological neighbours are words that can be formed from a given word by substituting, adding, or deleting one phoneme\" e.g. dog (Phonological Neighbors: bog, hog, dig, log, etc.)* ([Zhang, Carlson, & Diaz, 2020](https://www.tandfonline.com/doi/abs/10.1080/23273798.2019.1686529?journalCode=plcp21#:~:text=Phonological%20neighbours%20are%20words%20that,(2008).)))\n",
    "- `Concreteness_Rating`: How \"concrete\" a word is. Concreteness is \"the extent to which a word denotes an object that can be experienced by the senses\" ([Nelson & Schreiber](https://www.sciencedirect.com/science/article/abs/pii/0749596X9290013N)) Higher scores mean higher concreteness.\n",
    "- `Age_of_Acquisition`: the typical age at which people learn this word. \n",
    "- `Female`: 0 for male, 1 for female\n",
    "- `State`: The abbreviation for which state the subject lives in.\n",
    "\n",
    "## Instructions\n",
    "1. *Build a KNN, Decision Tree, AND Logistic Regression model to predict whether or not someone recalled a word (`correct`) using all the other variables.*\n",
    "    - If a categorical variable has more than 2 levels, use `get_dummies()` to create dummy variables for it (covered in All The Stuff You Need to Know Python) \n",
    "    - use the `train_test_split()` to do an 90/10 split (make sure to use the SAME split when training all 3 models, do *not* re-split your data. We want each model to be trained on the same training set).\n",
    "    - Appropriately z-score your continuous/interval variables only\n",
    "    - For KNN, include only *continuous/interval* columns as predictors. For Decision Tree and Logistic Regression use ALL columns (other than `correct`).\n",
    "    - For KNN, choose K by using `GridSearchCV` and an appropriate range of potential K's.\n",
    "    - For Decision Trees, use `GridSearchCV` and an appropriate range of numbers to choose `min_samples_leaf`.\n",
    "    - Print at least 4 performance metrics (e.g. accuracy, recall, precision, f1score, roc_auc, plotting a confusion marix...or any of the metrics we discussed...etc.) for both your **train and test set**.\n",
    "    \n",
    "    \n",
    "2. *Evaluate Your Models (WRITE YOUR ANSWER IN MARKDOWN/TEXT CELL)*\n",
    "    - A) Using the performance metrics you printed in #1 **thoroughly discuss which model did best (if you *had* to pick one), how can you tell?** (make sure to contextualize the metrics you discuss instead of just saying \"it has a good sensitivity\". **Include a discussion of whether the model is overfit, underfit, or neither.**\n",
    "    - B) Discuss **whether, based on this performance, you'd \"put this model into production\"** (e.g. use it and trust it to predict future data). In your discussion be sure to consider the performance of this specific model (as discussed in A) as well as the context of this specific situation (is this making a high stakes prediction? Are false positives more important than false negatives or are they the same?). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R",
   "version": "3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
