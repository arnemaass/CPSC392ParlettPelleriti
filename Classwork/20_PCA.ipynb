{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,\\\n",
    " f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "Principal component analysis (PCA) is a method of *dimensionality reduction* that takes the information in multiple variables/predictors, and presents that information (or at least MOST of it) in a smaller number of features. This smaller number of features--called components--are all linear combinations of the original variables, and the features are created in a way (eigendecomposition, if you're interested!) that makes the 1st component contain the MOST variability in the data, the 2nd component contain the second most variability...and so on.\n",
    "\n",
    "This allows us to choose only a handful of features (usually the first N features) that contain *most* of the information from the original data. This is helpful becuase few features often means faster models.\n",
    "\n",
    "We discussed 2 ways to choose the number of components that you retain:\n",
    "\n",
    "\n",
    "* a) **The Elbow Method**: create a scree plot, and find the \"elbow\" of the graph. Retain all the components at and before the elbow.\n",
    "* b) **The Percentage Method**: specify a specific % of variance that's acceptable to retain (e.g. 95%), and retain enough components to achieve that %.\n",
    "    \n",
    "    \n",
    "    \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1crCW8BAFVEu50th9VhdJMYakZho03kp0\" width = 500px />\n",
    "\n",
    "# PCA Visually\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1wBXkUp2MNZSSrnXhHsqNYmdYhC9Bs6x1\" height = 200px />\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ZHMOB5oDdogQgBBqTTrKfEnexXjPwEWS\" height = 200px />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn`: A Simple PCA for Compression\n",
    "Let's use [this data](https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/penguins.csv) to perform PCA on penguin body features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "penguin = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/penguins.csv\")\n",
    "penguin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X\n",
    "feats = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "penguin_sub = penguin[feats]\n",
    "\n",
    "penguin_sub.dropna(inplace = True)\n",
    "penguin_sub.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "# empty pipeline\n",
    "z = make_column_transformer((StandardScaler(), feats),\n",
    "                            remainder = \"passthrough\")\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pipe_penguin = Pipeline([\n",
    "    (\"z\", z),\n",
    "    (\"pca\", pca)\n",
    "])\n",
    "\n",
    "# fit\n",
    "pipe_penguin.fit(penguin_sub)\n",
    "\n",
    "# scree/cumvar plot\n",
    "pcaDF = pd.DataFrame({\"expl_var\" :\n",
    "                      pipe_penguin.named_steps[\"pca\"].explained_variance_ratio_,\n",
    "                      \"pc\": range(1,5),\n",
    "                      \"cum_var\":\n",
    "                      pipe_penguin.named_steps[\"pca\"].explained_variance_ratio_.cumsum()})\n",
    "\n",
    "# scree\n",
    "print(ggplot(pcaDF, aes(x = \"pc\", y = \"expl_var\")) + geom_line() + geom_point() +\n",
    " theme_minimal() + labs(x = \"PCs\", y = \"Proportion of Explained Variance\", title = \"Scree Plot\"))\n",
    "\n",
    "# cumulative variance\n",
    "pcaDF0 = pd.concat([pcaDF, pd.DataFrame({\"pc\": [0], \"cum_var\": [0]})]) # add 0,0 for reference\n",
    "\n",
    "print(ggplot(pcaDF0, aes(x = \"pc\", y = \"cum_var\")) + geom_line() +\n",
    " geom_point() + geom_hline(yintercept = 0.95, color = \"red\", linetype = \"dashed\") +\n",
    " theme_minimal() + labs(x = \"Number of PCs\", y = \"Cumulative Proportion of Explained Variance\", title = \"Cumulative Variance Plot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_PCs = pd.DataFrame(pipe_penguin.transform(penguin_sub))\n",
    "penguin_PCs.columns = [\"PC\" + str(i+1) for i in range(len(feats))]\n",
    "\n",
    "penguin_PCs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn`: Using PCs as Predictors in a Supervised Model\n",
    "\n",
    "Using `data`, run PCA on the predictors (V1-V11), then build a model that uses the first 1,3,5,10, and 11 components.\n",
    "\n",
    "* How much variance do the 1,3,5,10, and 11 components cover?\n",
    "* How does each model do on train/test accuracy? How do they do compared to the full model?\n",
    "* What patterns do you observe, and why do you think those patterns exist?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/pcaLogit.csv\")\n",
    "data.head()\n",
    "\n",
    "# set up X \n",
    "feats = [\"V\" + str(i) for i in range(1,12)]\n",
    "X = data[feats]\n",
    "y = data[\"Outcome\"]\n",
    "\n",
    "# tts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.2)\n",
    "\n",
    "# empty pipeline\n",
    "z = make_column_transformer((StandardScaler(), feats),\n",
    "                            remainder = \"passthrough\")\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pipe_pca = Pipeline([\n",
    "    (\"z\", z),\n",
    "    (\"pca\", pca)\n",
    "])\n",
    "\n",
    "# fit\n",
    "pipe_pca.fit(X_train)\n",
    "\n",
    "# scree/cumvar plot\n",
    "pcaDF = pd.DataFrame({\"expl_var\" :\n",
    "                      pipe_pca.named_steps[\"pca\"].explained_variance_ratio_,\n",
    "                      \"pc\": range(1,12),\n",
    "                      \"cum_var\":\n",
    "                      pipe_pca.named_steps[\"pca\"].explained_variance_ratio_.cumsum()})\n",
    "\n",
    "# print(ggplot(pcaDF, aes(x = \"pc\", y = \"expl_var\")) + geom_line() + geom_point() +\n",
    "#  theme_minimal() + labs(x = \"PCs\", y = \"Proportion of Explained Variance\", title = \"Scree Plot\"))\n",
    "\n",
    "print(ggplot(pcaDF, aes(x = \"pc\", y = \"cum_var\")) + geom_line() +\n",
    " geom_point() + geom_hline(yintercept = 0.95, color = \"red\", linetype = \"dashed\") +\n",
    " theme_minimal() + labs(x = \"Number of PCs\", y = \"Cumulative Proportion of Explained Variance\", title = \"Cumulative Variance Plot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty pipeline\n",
    "z = make_column_transformer((StandardScaler(), feats),\n",
    "                            remainder = \"passthrough\")\n",
    "\n",
    "pca = PCA(n_components = 11)\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    (\"z\", z),\n",
    "    (\"pca\", pca),\n",
    "    (\"model\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# fit\n",
    "pipe_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = pipe_lr.predict(X_train)\n",
    "y_pred_test  = pipe_lr.predict(X_test)\n",
    "\n",
    "y_pred_train_prob = pipe_lr.predict_proba(X_train)[:,1]\n",
    "y_pred_test_prob  = pipe_lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "# assess\n",
    "print(\"Train Acc       : \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Train Prescision: \", precision_score(y_train, y_pred_train))\n",
    "print(\"Train Recall    : \", recall_score(y_train, y_pred_train))\n",
    "print(\"Train F1        : \", f1_score(y_train, y_pred_train))\n",
    "print(\"Train ROC AUC   : \", roc_auc_score(y_train, y_pred_train_prob))\n",
    "\n",
    "\n",
    "print(\"Test Acc        : \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Test Prescision : \", precision_score(y_test, y_pred_test))\n",
    "print(\"Test Recall     : \", recall_score(y_test, y_pred_test))\n",
    "print(\"Test F1         : \", f1_score(y_test, y_pred_test))\n",
    "print(\"Test ROC AUC    : \", roc_auc_score(y_test, y_pred_test_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork\n",
    "\n",
    "## PCA with different variable correlations\n",
    "\n",
    "You can grab all pairwise correlations between variables/features in your model by using the command [`df.corr()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) where `df` is your dataframe, and `sb.heatmap(df.corr(), cmap=\"Blues\", annot=True)`.\n",
    "\n",
    "For the following datasets:\n",
    "\n",
    "1. Look at the correlations between all the variables in the dataframe. Are they high? low?\n",
    "2. Build an Empty Pipeline with Z scoring and PCA\n",
    "3. Fit the Model\n",
    "4. Make a Scree plot (be sure to add `+ ylim(0,1)`. What do you notice about the patterns in the screeplot? How do those relate to the correlations you saw?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n",
    "\n",
    "\n",
    "TIP: to make the scree plot more clear, it can help to add the row [0,0,0] to your dataframe of the explained/cumulative variance. You can do this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE WON'T RUN, it's just an example of how to do this\n",
    "pcaDF = pd.DataFrame({\"expl_var\" : pipe.named_steps[\"pca\"].explained_variance_ratio_, \"pc\": range(1,12),\n",
    "                      \"cum_var\": pipe.named_steps[\"pca\"].explained_variance_ratio_.cumsum()})\n",
    "\n",
    "# add zeros\n",
    "pcaDF = pcaDF.append(pd.DataFrame({\"expl_var\" : [0], \"pc\": [0], \"cum_var\": [0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/pca0.csv\")\n",
    "d1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/pca5.csv\")\n",
    "d2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/pca9.csv\")\n",
    "d3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/pca10.csv\")\n",
    "d4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d5 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/pca11.csv\")\n",
    "d5.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this has different variable names and a different # of columns\n",
    "d6 = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/pca12.csv\")\n",
    "d6.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and clustering\n",
    "\n",
    "Using the McDonald's Nutritional Data found here: https://github.com/cmparlettpelleriti/CPSC392ParlettPelleriti/blob/master/Data/McMenu.csv (see [Kaggle](https://www.kaggle.com/mcdonalds/nutrition-facts/version/1) for more info), use k-means with k = 4 to cluster the foods using all the variables except category, item, and serving size (make sure to z score variables first!).\n",
    "\n",
    "Next, perform PCA on all the variables except category, item, and serving size. Calculate and grab the first 2 PCs, and add them to your dataframe.\n",
    "\n",
    "Normally (like in your project) when we have more than 2 variables, we have to make MULTIPLE plots. But, one other option is to plot the first 2 PCs, and then color by cluster. Even though we are losing *some* information, we're still roughly able to see how cohesive/dense/separate our clusters are! Try making a scatterplot using the first 2 PCs, then coloring by cluster. What can you tell about your clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "d = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/McMenu.csv\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ####\n",
    "\n",
    "# grab columns we want to use\n",
    "names = [n for n in d.columns if n not in [\"Category\", \"Item\", \"Serving Size\"]]\n",
    "\n",
    "# X\n",
    "X = d[names]\n",
    "\n",
    "# create empty pipeline\n",
    "pipe_pca = ???\n",
    "\n",
    "# fit pipeline and grab cluster assignments\n",
    "labels = ???\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty pipeline\n",
    "\n",
    "\n",
    "# fit pipeline\n",
    "\n",
    "\n",
    "# grab 2 components and add them to d\n",
    "d[\"clusters\"] = labels\n",
    "d[[\"pc1\", \"pc2\"]] = pd.DataFrame(pipe_pca.transform(d[names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "### /YOUR CODE HERE ####"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
