{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "# models\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor # Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor # gradient boosting\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "\n",
    "# model validation\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n",
    "\n",
    "# performance\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,\\\n",
    " f1_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# pipeline imports\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "## Models\n",
    "\n",
    "|                     | **Decision Tree**         | **Random Forest**                  | **Gradient Boosting Tree** |\n",
    "|---------------------|---------------------------|------------------------------------|----------------------------|\n",
    "| _num trees_         | one                       | many                               | many                       |\n",
    "| _make predictions_  | mode or mean of leaf node | each tree votes                    | sum of tree outputs        |\n",
    "| _tree independence_ | NOT applicable            | independent                        | dependent                  |\n",
    "| _Data Used_         | all                       | bagging + random feature selection | all                        |\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1PgupmQER9-enXETnyxbLTihgub5B5_Bm\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1G0iSNeEwvhiTiMwW2jS1HPjo_2Wt_OOn\" alt=\"Q\" width = \"600\"/>\n",
    "</p>\n",
    "\n",
    "## Time and Space Complexity ✨ Vibes ✨\n",
    "- Think about how a prediction is made in a Decision (or Regression) Tree. Does the **depth** of the tree affect how *long* it takes to make a prediction? Is the prediction slower, faster, or the same with more depth?\n",
    "- How does the number of **nodes** in a tree affect the amount of values we need to *store* to make a prediction using a trained Decision Tree? Does a tree with more nodes take more memory to store, less memory, or the same?\n",
    "- When we build a Random Forest, will making predictions be slower, faster, or the same speed? Will it take more, the same, or less memory than storing a Decision Tree?\n",
    "- When we build a Gradient Boosting Tree, will making predictions be slower, faster, or the same speed? Will it take more, the same, or less memory than storing a Decision Tree?\n",
    "- (Bonus: We've been talking about time and space complexity when making a *prediction*, but at training time, which could be faster: Random Forests or Gradient Boosting Trees?)\n",
    "\n",
    "## Decision Trees, Graphically\n",
    "\n",
    "Let's load in our penguin data set, and plot the bill length and bill depth for our three species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pengwing = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/penguins.csv\")\n",
    "pengwing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(pengwing, aes(x = \"bill_length_mm\", y = \"bill_depth_mm\", color = \"species\")) + geom_point()\n",
    " + theme_minimal()\n",
    " + labs(x = \"Bill Length (in mm)\", y = \"Bill Depth (in mm)\", title = \"Bill Length vs. Bill Depth by Species\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a decision tree based on bill length and bill depth to classify penguins as different species. First, we could split on Bill Depth and decide that any penguin with a depth less than 16.5 mm, should be classified as a Gentoo penguin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = # Your split value here\n",
    "\n",
    "(ggplot(pengwing, aes(x = \"bill_length_mm\", y = \"bill_depth_mm\", color = \"species\")) + geom_point()\n",
    " + theme_minimal()\n",
    " + labs(x = \"Bill Length (in mm)\", y = \"Bill Depth (in mm)\", title = \"Bill Length vs. Bill Depth by Species\")\n",
    " + geom_hline(yintercept = split1, size = 1, linetype = \"dashed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That bottom group looks GREAT. Now let's look at the top group. Most of the Chinstrap penguins have longer bill lengths. Let's say that if a penguin has a bill depth > 16.5mm, then we will split on bill length at 41.5 to separate the Adelie and Chinstrap penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2 = ## Your split value here\n",
    "\n",
    "(ggplot(pengwing, aes(x = \"bill_length_mm\", y = \"bill_depth_mm\", color = \"species\")) + geom_point()\n",
    " + theme_minimal()\n",
    " + labs(x = \"Bill Length (in mm)\", y = \"Bill Depth (in mm)\", title = \"Bill Length vs. Bill Depth by Species\")\n",
    " + geom_hline(yintercept = split1, size = 1, linetype = \"dashed\")\n",
    " + geom_segment(x = split2, xend = split2, y = split1, yend = 22, size = 0.6, linetype = \"dashed\", color = \"black\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built a (very short) decision tree! It would look like this.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1wMi2k4RI9RuYq-NcHRVm7b62p4UwUXSh\" width = \"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Entropy\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1XIXhz6AsrwjIjU2yeS9pSsHXU8bkJJms\" alt=\"Entropy Low\" style=\"width: 400px;\"/>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1WewfI95gICSwbTU9DiQ1lSv2-grHIvBJ\" alt=\"Entropy Hight\" style=\"width: 400px;\"/>\n",
    "</p>\n",
    "\n",
    "Entropy is a measure of disorder/chaos. We want ordered and organized data in the leaf nodes of our decision trees. So we want LOW entropy. **Entropy** is defined as:\n",
    "\n",
    "$$ E = -\\sum_1^N p_i* log_2(p_i) $$\n",
    "\n",
    "Where $N$ is the number of categories or labels in our outcome variable.\n",
    "\n",
    "This is compared to **gini impurity** which is:\n",
    "\n",
    "$$GI = 1 - \\sum_1^N p_i^2$$\n",
    "\n",
    "Gini impurity is probability of misclassifying a random data point from that node.\n",
    "\n",
    "\n",
    "(if you're super into decision trees, check out this paper [Theoretical comparison between the Gini Index and\n",
    "Information Gain criteria](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf))\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1z75AaS4i_39n-sZrrkgYKHrm_e8p6GJR\" />\n",
    "</p>\n",
    "\n",
    "### *Question*\n",
    "\n",
    "WHY do we want the leaf nodes of our tree to be ordered (have low entropy or impurity?)?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Chaos for a Split\n",
    "\n",
    "When you split a node, we now have two new nodes. In order to calculate the chaos (entropy or gini impurity) of the split, we have to calculate the chaos (entropy or gini impurity) for EACH of the new nodes and then calculate the weighted average chaos (entropy or gini impurity).  \n",
    "\n",
    "The reason we weight each node differently in this calculation, is because if a node has more data in it, than it has more impact, and therefore its measure of chaos (entropy or gini impurity) should count more.\n",
    "\n",
    "In general, once you've calculated the chaos (entropy or gini impurity) for each of the new nodes, you'll use this formula to calculate the weighted average:\n",
    "\n",
    "\n",
    "$$ WC = (\\frac{N_L}{Total}* C_L) + (\\frac{N_R}{Total}* C_R)$$\n",
    "\n",
    "Where $N_L$ is the number of data points in the Left Node, $N_R$ is the number of data points in the Right Node, and $Total$ is the total number of data points in that split. $C_R$ and $C_L$ are the chaos measure (entropy of gini impurity) for the right and left nodes, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn`\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "Let's first build a Decision Tree to **classify** patients as diabetic or not diabetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/diabetes2.csv\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictors = [\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n",
    "              \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]\n",
    "X = d[predictors]\n",
    "y = d[\"Outcome\"]\n",
    "\n",
    "# z scoring not important, because none of the variables are influencing/compared to each other directly\n",
    "# scale doesn't matter here. But z scoring wont hurt.\n",
    "\n",
    "\n",
    "# TTS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1234)\n",
    "\n",
    "# Create Empty Model\n",
    "pre = make_column_transformer((StandardScaler(), predictors),\n",
    "                              remainder = \"passthrough\")\n",
    "tree = DecisionTreeClassifier(random_state = 1234)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", pre),\n",
    "    (\"tree\", tree)\n",
    "])\n",
    "\n",
    "# Fit\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ConfusionMatrixDisplay.from_predictions(y_test, pipe.predict(X_test)))\n",
    "print(ConfusionMatrixDisplay.from_predictions(y_train, pipe.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new model\n",
    "pipe.named_steps[\"tree\"].get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting Max_Depth\n",
    "We talked about different ways to \"prune\" or limit the depth of a tree, both directly and indirectly via `max_depth` and `min_samples_leaf`.\n",
    "\n",
    "The graph below shows the results of a [simulation](https://colab.research.google.com/drive/1fE8AyJVjh0Gsr2s2Co8-u3_Cwm1FSMJC?usp=sharing) looking at the effect of limiting the `max_depth` of decision trees.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=19r_NEZk0ETr_ifaxKgkWl6PxVYaQcYOt\" alt=\"biasvartrees\" width = \"600\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "#### Question\n",
    "\n",
    "What does this graph tell you about the effect of limiting the depth of trees?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = \"200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests and Gradient Boosting Trees\n",
    "\n",
    "Now let's copy and paste the code from above and build a **Random Forest** to predict diabetes instead of a single tree, and then using a **Gradient Boosting Tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictors = [\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n",
    "              \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]\n",
    "X = d[predictors]\n",
    "y = d[\"Outcome\"]\n",
    "\n",
    "# z scoring not important, because none of the variables are influencing/compared to each other directly\n",
    "# scale doesn't matter here. But z scoring wont hurt.\n",
    "\n",
    "\n",
    "# TTS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1234)\n",
    "\n",
    "# create empty model\n",
    "pre = make_column_transformer((StandardScaler(), predictors),\n",
    "                              remainder = \"passthrough\")\n",
    "tree = DecisionTreeClassifier(random_state = 1234)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", pre),\n",
    "    (\"tree\", tree)\n",
    "])\n",
    "\n",
    "# fit\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# predict/assess\n",
    "print(ConfusionMatrixDisplay.from_predictions(y_train, pipe.predict(X_train)))\n",
    "print(ConfusionMatrixDisplay.from_predictions(y_test, pipe.predict(X_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Trees for Regression\n",
    "Lastly, let's take a quick look at what we'd need to change if we wanted to predict a *continuous* value instead of a categorical one. Let's look at this data set that measures risk propensity. We're going to predict BART Scores (a score where higher values mean you're riskier), based on a bunch of different measures. We're going to change it from a  `LinearRegression()` model to a `DecisionTreeRegressor()`, `RandomForestRegressor()`, and `GradientBosstingRegressor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSEs: [145.93651722105184, 142.48796285982044, 140.5132561929619, 144.40330408041692, 140.0981168068198]\n",
      "Test MSEs : [133.3976045088573, 146.5382997694144, 154.499148658406, 139.7296199743544, 156.12355180229585]\n",
      "Train MSE : 142.68783143221418\n",
      "Test MSE  : 146.0576449426656\n",
      "Train MAEs: [10.674637323316684, 10.466649234978094, 10.352760898571537, 10.553602583404016, 10.298972364613958]\n",
      "Test MAEs : [10.005449109924223, 10.485291324542843, 10.847447491804708, 10.49856806876055, 11.109985888307044]\n",
      "Train MAE : 10.469324480976859\n",
      "Test MAE  : 10.589348376667873\n",
      "Train MAPEs: [0.8915441779450712, 0.8746255379828396, 0.8933940570011268, 0.862136421361214, 0.8560565751036264]\n",
      "Test MAPEs : [0.8708200061962684, 0.8824044237149958, 0.7106093507338618, 1.0552412127096522, 0.9230028558533473]\n",
      "Train MAPE : 0.8755513538787756\n",
      "Test MAPE  : 0.8884155698416251\n",
      "Train R2s: [0.05713992144949276, 0.0639962348339419, 0.0700885175706274, 0.058728792369517, 0.07354490730823704]\n",
      "Test R2s : [0.07144417471150344, 0.047348727675951774, 0.008668041377555835, 0.016033520745724394, 0.013019818847430642]\n",
      "Train R2 : 0.06469967470636322\n",
      "Test R2  : 0.03130285667163322\n"
     ]
    }
   ],
   "source": [
    "bart = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/bis-bas-bart-syn-clean.csv\")\n",
    "\n",
    "# drop missing\n",
    "bart.dropna(inplace = True)\n",
    "bart.reset_index(inplace = True)\n",
    "\n",
    "# X and y\n",
    "predictors = [c for c in bart.columns if c != \"BART\"]\n",
    "contin = [c for c in predictors if c != \"Female\"]\n",
    "\n",
    "X = bart[predictors]\n",
    "y = bart[\"BART\"]\n",
    "\n",
    "# model\n",
    "z = make_column_transformer((StandardScaler(), contin),\n",
    "                            remainder = \"passthrough\")\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "pipe = Pipeline([(\"zscore\", z),\n",
    "                (\"linearregression\", lr)])\n",
    "\n",
    "# model validation\n",
    "kf = KFold(5)\n",
    "\n",
    "mse = {\"train\": [], \"test\": []}\n",
    "mae = {\"train\": [], \"test\": []}\n",
    "mape = {\"train\": [], \"test\": []}\n",
    "r2 = {\"train\": [], \"test\": []}\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    X_train = X.iloc[train]\n",
    "    X_test  = X.iloc[test]\n",
    "    y_train = y[train]\n",
    "    y_test  = y[test]\n",
    "\n",
    "    # fit\n",
    "    pipe.fit(X_train,y_train)\n",
    "\n",
    "    # predict\n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "\n",
    "    # assess\n",
    "    mse[\"train\"].append(mean_squared_error(y_train,y_pred_train))\n",
    "    mse[\"test\"].append(mean_squared_error(y_test,y_pred_test))\n",
    "    \n",
    "    mae[\"train\"].append(mean_absolute_error(y_train,y_pred_train))\n",
    "    mae[\"test\"].append(mean_absolute_error(y_test,y_pred_test))\n",
    "\n",
    "    mape[\"train\"].append(mean_absolute_percentage_error(y_train,y_pred_train))\n",
    "    mape[\"test\"].append(mean_absolute_percentage_error(y_test,y_pred_test))\n",
    "\n",
    "    r2[\"train\"].append(r2_score(y_train,y_pred_train))\n",
    "    r2[\"test\"].append(r2_score(y_test,y_pred_test))\n",
    "\n",
    "print(\"Train MSEs:\", mse[\"train\"])\n",
    "print(\"Test MSEs :\", mse[\"test\"])\n",
    "print(\"Train MSE :\", np.mean(mse[\"train\"]))\n",
    "print(\"Test MSE  :\", np.mean(mse[\"test\"]))\n",
    "\n",
    "print(\"Train MAEs:\", mae[\"train\"])\n",
    "print(\"Test MAEs :\", mae[\"test\"])\n",
    "print(\"Train MAE :\", np.mean(mae[\"train\"]))\n",
    "print(\"Test MAE  :\", np.mean(mae[\"test\"]))\n",
    "\n",
    "print(\"Train MAPEs:\", mape[\"train\"])\n",
    "print(\"Test MAPEs :\", mape[\"test\"])\n",
    "print(\"Train MAPE :\", np.mean(mape[\"train\"]))\n",
    "print(\"Test MAPE  :\", np.mean(mape[\"test\"]))\n",
    "\n",
    "print(\"Train R2s:\", r2[\"train\"])\n",
    "print(\"Test R2s :\", r2[\"test\"])\n",
    "print(\"Train R2 :\", np.mean(r2[\"train\"]))\n",
    "print(\"Test R2  :\", np.mean(r2[\"test\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork\n",
    "## Gini Impurity\n",
    "\n",
    "Use python and numpy to write two functions, as described in the comments below.\n",
    "\n",
    "- LNP: Left Node Positive (cases)\n",
    "- LNN: Left Node Negative (cases)\n",
    "- RNP: Right Node Positive (cases)\n",
    "- RNN: Right Node Negative (cases)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src = \"https://drive.google.com/uc?id=1MQEeJDxxcV8zmhzBgaDZ2QY0Ng8z8hz8\" width = 300px/>\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Formulas\n",
    "\n",
    "$$GI = 1 - \\sum_1^N p_i^2$$\n",
    "\n",
    "Where $N$ is the number of categories or labels in our outcome variable.\n",
    "\n",
    "$$ WC = (\\frac{N_L}{Total}* C_L) + (\\frac{N_R}{Total}* C_R)$$\n",
    "\n",
    "Where $N_L$ is the number of data points in the Left Node, $N_R$ is the number of data points in the Right Node, and $Total$ is the total number of data points in that split. $C_R$ and $C_L$ are the chaos measure (entropy of gini impurity) for the right and left nodes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ############\n",
    "\n",
    "\n",
    "def gini():\n",
    "    # this function calculates the gini impurity for ONE node (left, right, or root!)\n",
    "    # this function should take in the POSITIVE cases and NEGATIVE cases as arguments\n",
    "    # and calculate the gini impurity for that node based on the formula above\n",
    "    # return the impurity for the node.\n",
    "    \n",
    "    pass\n",
    "\n",
    "def gini_split():\n",
    "    \n",
    "    # this function takes FOUR arguments: LNP, LNN, RNP, and RNN and calculates\n",
    "    # the gini impurity for each node (by calling gini()) and then calculates\n",
    "    # the WEIGHTED average of the impurity in each node.\n",
    "    # return the impurity for the split.\n",
    "    \n",
    "    pass\n",
    "\n",
    "### YOUR CODE HERE ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to test your code, if it prints True, you got the right answer\n",
    "\n",
    "abs(gini_split(10,5,2,12) - 0.3481116584564861) <= 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Use python and numpy to write two functions, as described by the comments below. If you want to read more about entropy, see this [article](https://bricaud.github.io/personal-blog/entropy-in-decision-trees/).\n",
    "\n",
    "hint: `np.log2()`\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Formulas\n",
    "\n",
    "$$ E = -\\sum_1^N p_i* log_2(p_i) $$\n",
    "\n",
    "Where $N$ is the number of categories or labels in our outcome variable.\n",
    "\n",
    "$$ WC = (\\frac{N_L}{Total}* C_L) + (\\frac{N_R}{Total}* C_R)$$\n",
    "\n",
    "Where $N_L$ is the number of data points in the Left Node, $N_R$ is the number of data points in the Right Node, and $Total$ is the total number of data points in that split. $C_R$ and $C_L$ are the chaos measure (entropy of gini impurity) for the right and left nodes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###############\n",
    "\n",
    "def entropy():\n",
    "    # this function calculates the entropy for ONE node (left, right, or root!)\n",
    "    # this function should take in the POSITIVE cases and NEGATIVE cases counts as arguments\n",
    "    # and calculate the entropy for that node based on the formula above\n",
    "    pass\n",
    "\n",
    "def entropy_split():\n",
    "    # this function takes FOUR arguments: LNP, LNN, RNP, and RNN and calculates\n",
    "    # the entropy for each node (by calling entropy()) and then calculates\n",
    "    # the WEIGHTED average of the entropy in each node.\n",
    "    # return the entropy for the split.\n",
    "    pass\n",
    "\n",
    "### YOUR CODE HERE ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to test your code, if it prints True, you got the right answer\n",
    "\n",
    "abs(entropy_split(10,5,2,12) - 0.7606157383093077) <= 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Categorical Decision Tree\n",
    "\n",
    "This dataset from UCI is about edible (`e`) and poisonous (`p`) mushrooms.\n",
    "\n",
    "- `gill-size`: `b` is for broad gills, `n` is for narrow gills.\n",
    "- `bruises`: `t` is for true, there are bruises, `f` for false, there are no bruises.\n",
    "- `poison`: `e` for edible, `p` for poison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poison</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>b</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>l</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>s</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>w</td>\n",
       "      <td>b</td>\n",
       "      <td>k</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>a</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>a</td>\n",
       "      <td>f</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>n</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>o</td>\n",
       "      <td>p</td>\n",
       "      <td>k</td>\n",
       "      <td>n</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  poison cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
       "0      e         x           s         y       t    a               f   \n",
       "1      e         b           s         w       t    l               f   \n",
       "2      p         x           y         w       t    p               f   \n",
       "3      e         x           s         g       f    n               f   \n",
       "4      e         x           y         y       t    a               f   \n",
       "\n",
       "  gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
       "0            c         b          k  ...                        s   \n",
       "1            c         b          n  ...                        s   \n",
       "2            c         n          n  ...                        s   \n",
       "3            w         b          k  ...                        s   \n",
       "4            c         b          n  ...                        s   \n",
       "\n",
       "  stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
       "0                      w                      w         p          w   \n",
       "1                      w                      w         p          w   \n",
       "2                      w                      w         p          w   \n",
       "3                      w                      w         p          w   \n",
       "4                      w                      w         p          w   \n",
       "\n",
       "  ring-number ring-type spore-print-color population habitat  \n",
       "0           o         p                 n          n       g  \n",
       "1           o         p                 n          n       m  \n",
       "2           o         p                 k          s       u  \n",
       "3           o         e                 n          a       g  \n",
       "4           o         p                 k          n       g  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Mushroom Data\n",
    "\n",
    "# see this site for what variables mean: http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "mush = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\")\n",
    "\n",
    "mush.columns = ['poison','cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\n",
    "                'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\n",
    "                'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number','ring-type',\n",
    "                'spore-print-color', 'population', 'habitat']\n",
    "\n",
    "mush.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your sanity, let's restrict our dataset to 3 predictor variables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mush_small = mush[[\"poison\", \"bruises\", \"gill-size\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build!\n",
    "\n",
    "Use the functions you built earlier to build a (very small) decision tree that classifies each data point as either edible (`e`) or poisonous (`p`). You can choose to either use entropy or gini impurity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 1\n",
    "\n",
    "Choose which variable to use to split the **first layer**. You have three options: leave the root node alone, split on gill-size, or split on bruises.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no split\n",
    "poison_split = {\"e\": np.sum(mush_small.poison == \"e\"),\n",
    "                \"p\": np.sum(mush_small.poison == \"p\")}\n",
    "\n",
    "poison_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bruise split\n",
    "bruise_NodeF = mush_small.loc[mush_small.bruises == \"f\"] #node with bruise = F\n",
    "bruise_NodeT = mush_small.loc[mush_small.bruises == \"t\"] #node with bruise = T\n",
    "\n",
    "bruise_split = {\"f\": {\"e\": bruise_NodeF[bruise_NodeF.poison == \"e\"].shape[0],\n",
    "                              \"p\": bruise_NodeF.loc[bruise_NodeF.poison == \"p\"].shape[0]},\n",
    "                        \"t\": {\"e\": bruise_NodeT[bruise_NodeT.poison == \"e\"].shape[0],\n",
    "                              \"p\": bruise_NodeT.loc[bruise_NodeT.poison == \"p\"].shape[0]},}\n",
    "\n",
    "bruise_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gill_NodeB = mush_small.loc[mush_small[\"gill-size\"] == \"b\"] #node with gill = b\n",
    "gill_NodeN = mush_small.loc[mush_small[\"gill-size\"] == \"n\"] #node with gill = n\n",
    "\n",
    "gill_split = {\"b\": {\"e\": gill_NodeB[gill_NodeB.poison == \"e\"].shape[0],\n",
    "                              \"p\": gill_NodeB.loc[gill_NodeB.poison == \"p\"].shape[0]},\n",
    "                        \"n\": {\"e\": gill_NodeN[gill_NodeN.poison == \"e\"].shape[0],\n",
    "                              \"p\": gill_NodeN.loc[gill_NodeN.poison == \"p\"].shape[0]},}\n",
    "\n",
    "gill_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate impurity/entropy of each possible split using your functions\n",
    "\n",
    "# 1. no split (impurity/entropy of root node)\n",
    "\n",
    "\n",
    "# 2. split on bruise (impurity/entropy of bruise node)\n",
    "\n",
    "\n",
    "# 3. split on gill-size (impurity/entropy of gill node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which split improves prediction most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question*\n",
    "\n",
    "Does splitting the root node improve the tree? How can you tell?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create Classifications\n",
    "\n",
    "Pretend that this decision stump (a decision tree with only one layer, selected in the previous part) is your final tree. Generate the classification for each data point and store it in `mush_small`. You should end up with a column where the value is `e` if your predicted the mushroom is edible, and `p` if you predicted the model is poisonous.\n",
    "\n",
    "\n",
    "Remember, once you have chosen your split, we predict that the data point in each node is whatever class is most common in that node. For example, if you did no splits, and just used the root node, we would predict that all mushrooms are edible (`e`) because it is the most common in the root node (`{'e': 4208, 'p': 3915}`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaos\n",
    "\n",
    "### *Question*\n",
    "\n",
    "When would Gini Impurity be 0? When would Entropy be 0? What does that mean about our tree/node?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
