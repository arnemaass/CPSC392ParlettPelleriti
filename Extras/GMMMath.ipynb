{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Models Generally\n",
    "\n",
    "Mixture models are generally defined as: \n",
    "\n",
    "$$p(x) = \\sum_{k = 1}^{K}\\pi_k p_k(x)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $K$ is the number of groups/clusters\n",
    "- $\\pi_k$ is the weight showing how common group $K$ is ($0 \\leq \\pi_k \\leq 1$ and $\\sum_{k = 1}^K \\pi_k  = 1$)\n",
    "- $p_k(x)$ is the probability of $x$ for the k'th distribution\n",
    "\n",
    "This equation tells us that our data are a combination of $K$ different groups with weights $\\pi_k$ and probability distributions $p_k(x)$\n",
    "\n",
    "In our lecture, we saw a simple example where our data was made up of 2 different normal (gaussian) distributions.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1DY1xFHTE2spjA7owrMy55jiANPOlKoXL\" alt=\"Q\"/>\n",
    "\n",
    "This would have two groups ($K$ = 2). Each group is normally distributed ($p_1(x) = N(\\mu_1, \\Sigma_1), p_2(x) =  N(\\mu_2, \\Sigma_2)$) with equal weights ($\\pi_1 = \\pi_2 = 0.5$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Application to GMM\n",
    "\n",
    "We just learned that mixture models say that data come from multiple ($K$) distributions ($p_k(x)$), each with different weights ($\\pi_k$) based on how common each distribution is in the data.\n",
    "\n",
    "Gaussian Mixture Modelling uses this idea to create clusters from our data. We pick $K$ (called `n_components` in sklearn), and ask sklearn to figure out\n",
    "\n",
    "- what the clusters **are** (we assume they're multivariate normal, but ask sklearn to figure out the mean ($\\mu_k$) and variance ($\\Sigma_k$) of each one).\n",
    "- the **probability** that each of our data points belongs to each cluster.\n",
    "\n",
    "\n",
    "Officially, the algorithm we use is called the EM (Expectation-Maximization) algorithm, but it is pretty similar to how we found clusters in K-Means. \n",
    "\n",
    "Remember, in K-Means our steps were:\n",
    "\n",
    "1. Randomly initiallize the centers of clusters\n",
    "2. Assign every data point to it's closest cluster\n",
    "3. Use that cluster assignment to move the center of the cluster\n",
    "4. Repeat 2 and 3 until we have convergence. \n",
    "\n",
    "In GMM, we'll do the same thing BUT we have to account for the fact that 1) every data point belongs to every cluster and 2) we estimate the variance of each cluster instead of assuming its spherical.\n",
    "\n",
    "1. Randomly initialize the centers (means) and variances of clusters\n",
    "2. **E-Step**: Calculate the *responsibility* ($r_{nk}$) which tells us how likely a data point ($n$) is to be in a cluster ($k$)\n",
    "3. **M-Step**: Use these *responsibilities* to update the mean and variance of each cluster.\n",
    "4. Repeat 2 and 3 until we have convergence.\n",
    "\n",
    "Looks pretty similar! Let's talk about the math for steps 2 and 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The E-Step\n",
    "\n",
    "The E (or Expectation) Step to calculate the probability that a data point $n$ belongs to cluster $k$ for every data point, for every cluster. \n",
    "\n",
    "#### Question\n",
    "If we have 100 data points ($n = 100$), and 5 clusters ($k = 5$), how many *responsibilites* will we be calculating?\n",
    "\n",
    "\n",
    "FYI, We do this using the Normal/Gaussian density formula...\n",
    "\n",
    "$N(\\mu, \\Sigma)=\\left(\\frac{1}{2\\pi}\\right)^{p/2}|\\Sigma|^{-1/2}\\exp\\{-\\frac{1}{2}(\\textbf{x}-\\mathbf{\\mu})'\\Sigma^{-1}(\\textbf{x}-\\mathbf{\\mu})\\}$\n",
    "\n",
    "...but you don't need to memorize this density formula. Computers will do this part of the calculation for us.\n",
    "\n",
    "### Responsibilities\n",
    "The responsibilities ($r_{nk}$) tell us the probability that a data point $n$ belongs to cluster $k$. It is calculated as: \n",
    "\n",
    "$$ r_{nk} = \\frac{\\pi_k N(x_n |\\mu_k, \\Sigma_k)}{\\sum_j \\pi_j N(x_n | \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "$\\pi_k N(x_n | \\mu_k, \\Sigma_k)$ is the weight for cluster $k$ ($\\pi_k$) times the probability of data point $x_n$ being from cluster $k$ (which we calculate using that multivariate normal density function above).\n",
    "\n",
    "$$\\underbrace{\\pi_k}_\\text{cluster $k$ weight} \\underbrace{N(x_n | \\mu_k, \\Sigma_k)}_\\text{probability of $x_n$ for cluster $k$}$$\n",
    "\n",
    "\n",
    "It's important to include $\\pi_k$ because if cluster $k$ has a lot of data points, than any given data point is pretty likely to be in cluster $k$. It's important to include $N(x_n | \\mu_k, \\Sigma_k)$ because it tells us whether a data point *is a good fit* with that cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The M-Step\n",
    "Once we've calculated all the *responsibilities*, we use them to update the means ($\\mu_k$) and variances ($\\Sigma_k$) of each of our clusters. \n",
    "\n",
    "Because every data point belongs to every cluster, every data point is used to estimate the mean and variance of each cluster. BUT not every data point is equally as influential. If a data point has a low chance of being in cluster 1, then it won't have much influence on the mean and variance of cluster 1. \n",
    "\n",
    "**Data points that have a high probability of being in a cluster have the most influence over its mean and variance** this is unlike K-Means where every data point in a cluster is equally influential.\n",
    "\n",
    "### Updating Means\n",
    "\n",
    "The formula for updating the mean of cluster $k$ is: \n",
    "\n",
    "$$\\mu_k = \\frac{1}{N_k}\\sum_{n = 1}^N r_{nk}x_n$$\n",
    "\n",
    "Compare that to the formula for calculating a regular mean (like in K-Means):\n",
    "\n",
    "$$\\mu_k = \\frac{1}{N}\\sum_{n = 1}^N x_n$$\n",
    "\n",
    "You can see that each data point ($X_n$) is weighted by its responsibility $r_{nk}$. This is *why* data points with higher responsibilities have more influence.\n",
    "\n",
    "The fraction at the begininning ($\\frac{1}{N_k}$) is the sum of all the responsibilities for that cluster. Basically, it's a way of estimating how many data points are in a cluster when data points belong to more than one cluster ($N_k = \\sum_{n = 1}^N r_{nk}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Variances\n",
    "The formula for updating the variance of cluster $k$ is:\n",
    "$$\\Sigma_k = \\frac{1}{N_k} \\sum_{n = 1}^N r_{nk}(x_n-\\mu_k)(x_n - \\mu_k)^T$$\n",
    "\n",
    "Compare this to the regular formula for calculating varinces:\n",
    "$$\\Sigma_k = \\frac{1}{N} \\sum_{n = 1}^N (x_n-\\mu_k)(x_n - \\mu_k)^T$$\n",
    "Again, this looks like the typical formula for variance, except each data point is weighted by its responsibility.\n",
    "\n",
    "\n",
    "### Updating $\\pi_k$\n",
    "\n",
    "The formula for updating the weights of each cluster is:\n",
    "\n",
    "$$\\pi_k = \\frac{N_k}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple, Univariate Example\n",
    "\n",
    "data = 1,2,4,7,8,10\n",
    "\n",
    "k = 2\n",
    "\n",
    "### Step 1. Randomly Initialize Distributions\n",
    "\n",
    "Let's assume:\n",
    "\n",
    "Cluster 1: $p_1(x) = N(3,1)$, $\\pi_1 = 0.5$\n",
    "\n",
    "Cluster 2: $p_2(x) = N(6,1)$, $\\pi_2 = 0.5$\n",
    "\n",
    "### Step 2: E-Step\n",
    "\n",
    "Now lets calculate the *responsibilities* of each data point. The function `sp.norm.pdf()` will calculate $N(\\mu_k, \\sigma_k)$ for us. \n",
    "\n",
    "\n",
    "|    | p(x \\| mu = 3, var = 1) | p(x \\| mu = 6, var = 1) |\n",
    "|----|-------------------------|-------------------------|\n",
    "| 1  | 0.05399                 | 0.00001                 |\n",
    "| 2  | 0.24197                 | 0.00013                 |\n",
    "| 4  | 0.24197                 | 0.05399                 |\n",
    "| 7  | 0.00013                 | 0.24197                 |\n",
    "| 8  | 0.00001                 | 0.05399                 |\n",
    "| 10 | 0.00001                 | 0.00013                 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sp\n",
    "\n",
    "data = [1,2,4,7,8,10]\n",
    "\n",
    "# calculate the responsibilities\n",
    "\n",
    "responsibilities = {\"Cluster1\": [],\n",
    "\"Cluster2\":[]}\n",
    "\n",
    "for d in data:\n",
    "    # get the p_k(x) for both clusters\n",
    "    p_k1 = #\n",
    "    p_k2 = #\n",
    "    # sp.norm.pdf(#, loc = #, scale = #)\n",
    "\n",
    "    # calculate the responsibilities\n",
    "    respon1 = # r_{n1}\n",
    "    respon2 = # r_{n2}\n",
    "\n",
    "    # store responsibilities in the dictionary\n",
    "    responsibilities[\"Cluster1\"].append(respon1)\n",
    "    responsibilities[\"Cluster2\"].append(respon2)\n",
    "\n",
    "print(responsibilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: M-Step\n",
    "\n",
    "Now let's update $\\mu_k$, $\\Sigma_k$ and $\\pi_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means\n",
    "\n",
    "## cluster 1 mean\n",
    "sum1 = 0 # sum of data * responsibilities\n",
    "nk1 = 0 # Nk1\n",
    "sum2 = 0 # sum of data * responsibilities\n",
    "nk2 = 0 # Nk2\n",
    "\n",
    "for i in range(len(data)):\n",
    "    # add the product of x and responsibility (cluster 1)\n",
    "    \n",
    "\n",
    "    #sum up the responsibilites for Nk1\n",
    "    \n",
    "\n",
    "    # add the product of x and responsibility (cluster 2)\n",
    "    \n",
    "\n",
    "    #sum up the responsibilites for Nk2\n",
    "    \n",
    "\n",
    "# calculate means\n",
    "mu1 = #\n",
    "mu2 = #\n",
    "print(\"means    :\", round(mu1,3), round(mu2,3))\n",
    "\n",
    "\n",
    "# variances \n",
    "var1 = 0\n",
    "var2 = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    # add the product of the responsibility times (x-mu)**2\n",
    "    \n",
    "\n",
    "# calculate the varinces\n",
    "var1 = #\n",
    "var2 = #\n",
    "print(\"variances:\",round(var1,3),round(var2,3))\n",
    "\n",
    "# pi_ks\n",
    "\n",
    "pi1 = #\n",
    "pi2 = #\n",
    "\n",
    "print(\"weights  :\",round(pi1,3),round(pi2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Repeat\n",
    "\n",
    "At this point, we would repeat the E and M steps using the new means, variances and $\\pi_k$ that we printed above.\n",
    "\n",
    "\n",
    "## Takeaways\n",
    "Luckily, computers usually do the math for us, but seeing the math helps us understand *conceptually* what the algorithms are doing. \n",
    "\n",
    "The biggest takeaways from this lesson are:\n",
    "\n",
    "- GMM does *soft assignment*, therefore (unlike K-Means) every data point has a probability of belonging to each cluster. Data points that are more likely to belong to a cluster have more *influence* on their means and variances. \n",
    "- GMM uses the EM algorithm to *iteratively* update the cluster distributions. It does this by first assinging a responsability to each data point (E-step), and then using them to calculate weighted means and variances for each cluster (M-step).\n",
    "- Responsibilities ($r_{nk}$) measure the probability of a data point being in each cluster (technically it is the *posterior* probability).\n",
    "- Responsibilities ($r_{nk}$) contain information about how common a cluster is ($\\pi_k$), as well as the likelihood of a data point belonging to that cluster ($N(\\mu_k, \\Sigma_k$))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
